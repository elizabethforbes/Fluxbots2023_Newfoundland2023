---
title: "LGR_fluxbot_comparison"
format: html
editor: visual
---

## LGR and fluxbot comparison: LGR plugged into fluxbot 20 chamber for direct comparison of fluxes, raw data collection

Import LGR data from same time period:

```{r}
lgrdat <- read.csv("lgr_coupled_chamber_stiched.csv", header = TRUE)

lgr <- lgrdat %>% 
  select(Time, X.CH4._ppm, X.H2O._ppm, X.CO2._ppm, GasP_torr, GasT_C, AmbT_C) %>% 
  rename(CH4ppm = X.CH4._ppm,
         H2Oppm = X.H2O._ppm,
         CO2ppm = X.CO2._ppm) %>% 
  mutate(Time = as.POSIXct(Time, format = "%m/%d/%Y %H:%M:%S"))

lgr %>% 
  filter(CO2ppm < 1000) %>%
  filter(Time > "2023-10-11 11:59:59" &
           Time < "2023-10-13 11:59:59") %>% 
  ggplot(aes(x=Time, y=CO2ppm))+
  geom_point(alpha = 0.5, color = "lightblue")+
  theme_classic()
```

## Merge the LGR and the fluxbot20 data from October:

```{r}
lgr_bot20 <- lgr %>% left_join(fluxbot_Oct, by = c("Time" = "timestamp"))
# Warning: Detected an unexpected many-to-many relationship between `x` and `y`.

# visualize:
lgr_bot20 %>% 
  filter(CO2ppm < 1500) %>% 
  # filter(co2 < 1500) %>% 
  # filter(Time > "2023-10-11 11:59:59" &
  #          Time < "2023-10-13 11:59:59") %>% 
  filter(Time > "2023-10-11 10:56:00" &
           Time < "2023-10-11 10:59:59") %>% # scroll through a few hours of data to get a sense of when the two streams align with each other for flux estimates
  ggplot(aes(x=Time, y=CO2ppm))+
  geom_point(alpha = 0.5, color = "darkgray")+
  geom_point(aes(y=co2), alpha = 0.5, color = "lightblue")+
  xlab("")+
  theme_classic()
```

Create 'bot_intervals' and 'lgr_intervals' dfs:

```{r}
bot_intervals <- lgr_bot20 %>% 
  select(Time, co2, humidity, pressure, tempC) %>% 
  drop_na()

lgr_intervals <- lgr_bot20 %>% 
  select(Time, CO2ppm, humidity, pressure, tempC) %>% 
  rename(co2 = CO2ppm) %>% 
  fill(c(humidity, pressure, tempC)) %>%  # backfill these columns with the same values til they reach a new one (bot data taken every sex seconds, so lgr data will have six cells in a row of the same H, P, and T data)
  drop_na()
  
```

Create df with custom observation intervals, LGR:

```{r}
write.csv(lgr_bot20, col.names = T, row.names = F, file = "lgr_bot20_compdata.csv")
fluxdat <- data("lgr_bot20_compdata.csv")
# 
# flux_lgr <- LoadOther(file = fluxdat,
#                       time = "Time",
#                       time_format = "ymd_HMS",
#                       CO2 = "CO2ppm",
#                       Ta = "AmbT_C")
# 
# flux_bot <- LoadOther(file = fluxdat,
#                       time = "Time",
#                       time_format = "ymd_HMS",
#                       CO2 = "co2",
#                       Ta = "tempC")
# flux_bot <- flux_bot %>% filter(!is.na(X.CO2.d_ppm)) %>% 
#   mutate(X.CO2.d_ppm = as.numeric(X.CO2.d_ppm))

# generate start/end time file:
start <- lgr_bot20 %>% 
  select(Time, AmbT_C, tempC) %>% 
  # generate date, hour, min:
  mutate(date = date(Time),
         hour = hour(Time),
         minute = minute(Time),
         sec = second(Time)) %>% 
  # mutate(hms = hms(c(hour, minute, sec))) %>% 
  filter(minute == 56 & sec == 0)

end <- lgr_bot20 %>% 
  select(Time, AmbT_C, tempC) %>% 
  # generate date, hour, min:
  mutate(date = date(Time),
         hour = hour(Time),
         minute = minute(Time),
         sec = second(Time)) %>% 
  # mutate(hms = hms(c(hour, minute, sec))) %>% 
  filter(minute == 59 & sec == 59)

startend <- rbind(start, end)

startend <- startend %>% 
  select(Time, minute) %>% 
  unique()
startend$row <- seq_len(nrow(startend))
startend <- 
  pivot_wider(data = startend,
              names_from = minute,
              values_from = Time)
startend$row <- NULL

startend_56 <- na.omit(startend$`56`)
startend_59 <- na.omit(startend$`59`)
startend <- cbind(startend_56, startend_59)
startend <- startend %>% 
  as_data_frame() %>% 
  rename(Start = startend_56, End = startend_59)

startend <- startend %>% 
  # mutate(Start = anytime(Start),
         # End = anytime(End)) %>% 
  mutate(Start = as_datetime(Start, tz = "America/New_York"),
         End = as_datetime(End, tz = "America/New_York"))

```

Create df with custom start-end intervals for bot dataset, using startend intervals generated for LGR data:

```{r}
library(dplyr)

# Create a function to find the closest timestamp
find_closest_timestamp <- function(target, timestamps) {
  closest_index <- which.min(abs(timestamps - target))
  return(timestamps[closest_index])
}

# Apply the function to find the closest timestamp in df2 for each start and end timestamp in df1
startend_bot_closest <- startend %>%
  rowwise() %>%
  mutate(
    closest_start = find_closest_timestamp(Start, bot_intervals$timestamp),
    closest_end = find_closest_timestamp(End, bot_intervals$timestamp)
  ) %>% 
  select(closest_start, closest_end) %>% 
  rename(Start = closest_start, End = closest_end) %>% 
  # remove any intervals with a difference less than 3s
  filter((End - Start) >= 3)
  
```

Custom function for calculating regressions of intervals using custom startend interval doc:

```{r}

calculate_regressions_custom_intervals <- function(time_series_df, custom_intervals_df) {
  # Extract timestamp and data columns from the time series dataframe
  timestamp_col <- as.POSIXct(time_series_df$Time, format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York")
  data_col <- time_series_df$co2     

  # Initialize an empty dataframe to store the results
  results_df <- data.frame(
    start_timestamp = character(),
    end_timestamp = character(),
    linear_beta = numeric(),
    quadratic_beta = numeric()
  )
  
  # Quality control thresholds:
    diff_threshold <- 5 #difference in start-end delta needs to be greater than 5ppm
    max_threshold <- 3000 #eliminate any intervals that contain co2 concentrations greater than 3k ppm
    length_threshold <- 15 #the length of an interval needs to be at least 25 observations (40 obs per 4mins interval for fluxbot)

  # Loop through each row in the custom intervals dataframe
  for (i in seq_len(nrow(custom_intervals_df))) {
    # Extract start and end points for the current row
    start_point <- custom_intervals_df$Start[i]
    end_point <- custom_intervals_df$End[i]

    # Find the closest timestamp to start_point and end_point
    start_index <- which.min(abs(timestamp_col - start_point))
    end_index <- which.min(abs(timestamp_col - end_point))

    # Extract the data for the current interval
    interval_data <- time_series_df$co2[start_index:end_index]
    interval_timestamp <- time_series_df$Time[start_index:end_index]
    
    # Quality control checks:
    if (abs(interval_data[1] - interval_data[length(interval_data)]) < diff_threshold) {
      cat("Skipping interval due to extremely small delta:", i, "\n")
      next  # Skip to the next iteration
    }
    if(max(interval_data) > max_threshold) {
      cat("Skipping interval: concentration too high:", i, "\n")
      next # Skip to the next iteration
    }
    if(length(interval_data) < length_threshold) {
      cat("Skipping interval due to insufficient length:", i, "\n")
      next # Skip to the next iteration
    }

    # Linear regression
    linear_model <- lm(interval_data ~ as.numeric(difftime(interval_timestamp, min(interval_timestamp), units = "secs")))
    linear_results <- summary(linear_model)
    linear_beta <- coef(linear_results)[2]  # Extract the beta coefficient

    # Quadratic regression
    quadratic_model <- lm(interval_data ~ 
                            as.numeric(difftime(interval_timestamp, min(interval_timestamp), 
                                                units = "secs")) +
                            I(as.numeric(difftime(interval_timestamp, 
                                                  min(interval_timestamp), 
                                                  units = "secs"))^2))
    quadratic_results <- summary(quadratic_model)
    quadratic_beta <- coef(quadratic_results)[2]  # Extract the beta coefficient

    # Append the results to the dataframe
    results_df <- rbind(results_df, data.frame(
      start_timestamp = start_point,
      end_timestamp = end_point,
      linear_beta = linear_beta,
      quadratic_beta = quadratic_beta
    ))
  }

  # Return the results
  return(results_df)
}


```

Custom function for estimating fluxes with conversion of CO2ppm to mass including a humidity correction:

```{r}
# function to convert iterative observation intervals of co2 accumulation over 4min time periods (in concentration, ppm) to units of co2 mass accumulation; the function uses ideal gas laws and a humidity correction to convert from ppm to kg co2. Once converted, the function then takes both a linear and quadratic regression of each accumulation interval, and uses the resultant slopes (beta values) to calculate a rate of increase in CO2 over time, per unit area.

# currently the function requires an input of the time series dataframe itself, a custom intervals dataframe (two columns, one labeled "Start" and one labeled "End" with each row indicating the ideal start and end of each interval in the time series df in POSIXct format), and an indication of the method (e.g. is it "LGR" or "fluxbot" generated data).

calculate_regressions_custom_intervals <- function(time_series_df, custom_intervals_df, method) {
  # Extract timestamp and data columns from the time series dataframe
  timestamp_col <- as.POSIXct(time_series_df$Time, format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York")
  data_col <- time_series_df$co2     

  # Initialize an empty dataframe to store the results
  results_df <- data.frame(
    start_timestamp = character(),
    end_timestamp = character(),
    linear_beta = numeric(),
    quadratic_beta = numeric()
  )
  
  # Quality control thresholds: if an interval 'fails' any of these it is skipped and excluded from the final output
    diff_threshold <- 5 # difference in co2 concentration delta from start-end needs to be greater than 5ppm
    max_threshold <- 3000 # eliminate any intervals that contain co2 concentrations greater than 3k ppm
    length_threshold <- 15 # the length of an interval needs to be at least 15 observations (40 obs per 4mins interval for fluxbot)
    
  # constants for conversion of concentration to mass:
    R_m = 8.314472  # Ideal gas constant for mass [(m3*Pa)/(K*mol)]
    R_specific <- 287.058 # specific gas constant for dry air (J/kg*K)
    mol_mass <- 28.9628 # molar mass dry air, g/mol
    volume <- 768 # cm3; volume of fluxbot chamber
    area <- 81 # cm2; area of soil that fluxbot collar encompasses
    g_per_mol = 44.009 # molar mass of CO2, g/mol

  # Loop through each row in the custom intervals dataframe to ID the start and end point of each interval
  for (i in seq_len(nrow(custom_intervals_df))) {
    # Extract start and end points for the current row
    start_point <- custom_intervals_df$Start[i]
    end_point <- custom_intervals_df$End[i]

    # Find the closest timestamp to start_point and end_point in the time series data itself:
    start_index <- which.min(abs(timestamp_col - start_point))
    end_index <- which.min(abs(timestamp_col - end_point))

    # Extract the CO2 concentration data for each interval iteratively using the above timestamps:
    interval_data <- time_series_df$co2[start_index:end_index]
    interval_timestamp <- time_series_df$Time[start_index:end_index]
    # determine total length of interval; reports in mins, so convert to sec:
    length_interval <- (as.numeric(time_series_df$Time[end_index] - time_series_df$Time[start_index]))*60
    
    # Quality control checks with printouts of reasons for excluding a given interval:
    if (any(is.na(interval_data))) {
      cat("Skipping interval due to NAs in the data:", i, "\n")
      next  # Skip to the next iteration
    }

    # Quality control checks:
    if (abs(interval_data[1] - interval_data[length(interval_data)]) < diff_threshold) {
      cat("Skipping interval due to extremely small delta:", i, "\n")
      next  # Skip to the next iteration
    }
    if(max(interval_data) > max_threshold) {
      cat("Skipping interval: concentration too high:", i, "\n")
      next # Skip to the next iteration
    }
    if(length(interval_data) < length_threshold) {
      cat("Skipping interval due to insufficient length:", i, "\n")
      next # Skip to the next iteration
    }
    
    # calculate rho_a, air density in kg/m3, including humidity:
    # first step, calculate saturation vapor pressure in hPa
    e_star <- 10*(0.61978 * exp((17.27 * time_series_df$tempC)[start_index:end_index]/
                                  (time_series_df$tempC[start_index:end_index] + 
                                     273.3))) # saturation vapor pressure, hPa
    # next: saturation density (kg water / kg air) given saturation vapor pressure (e_star) and observed air pressure
      # 0.62198 = ratio of molecular masses of water and dry air (18.01528 / 28.9645)
    x_s <- 0.62198 * e_star / (time_series_df$pressure[start_index:end_index] - e_star) 
    
    # next: calculate humidity ratio using saturation density and observed relative humidity:
    x <- x_s * time_series_df$humidity[start_index:end_index] / 100 # divide by 100 to convert to a fraction
    
    # finally, calculate the observed density of humidity-corrected air in the chamber:
    # first: density of air in kg/m3 using specific gas constant for dry air
    rho_d <- (time_series_df$pressure[start_index:end_index] * 100) / 
      (R_specific * (time_series_df$tempC[start_index:end_index] + 273.15)) # here converting T to Kelvin from Celcius
    # air density in kg/m3 with humidity correction as calculated in "x" above:
      # 1.609 = gas constant ratio between water vapor and air
    rho_a <- rho_d * (1+x) / (1 + 1.609 * x) 
    
    # convert (general) co2 in ppm to mol/m3
    mol_kg = 1/(mol_mass/1000) # mass of air in mol/kg
    # calculate mol of moist air per m3 in the chamber given previously-calculated moist air densities
    mol_m3 = mol_kg * rho_a # mass of moist air in chamber in mol/m3
    
    # convert (our observed) co2 in ppm to molar density using chamber volume:
    mol_gas_m3 <- mol_m3 * (interval_data/1000000) # returns mol/m3 (conversion from cm3 to m3 = 1,000,000)
  
    # convert molar concentration of CO2 into concentration in kg/m3
    kg_gas_m3 <- mol_gas_m3 * (g_per_mol/1000) # (conversion of g to kg = 1000)
      
    # convert to just mass in kg (volume reported in cm3, converting to m3 here, inverse conversion = 0.000001):
    kg_gas <- kg_gas_m3 * (volume * 0.000001) # returns kg of co2 in the chamber at each time point in interval

    # Linear regression
    linear_model <- lm(kg_gas ~ as.numeric(difftime(interval_timestamp, min(interval_timestamp), units = "secs")))
    linear_results <- summary(linear_model)
    linear_beta <- coef(linear_results)[2]  # Extract the beta coefficient; kg/s
    delta_kg_L = linear_beta*(length_interval) # units in kg
    delta_g_L = delta_kg_L*1000 # units in g
    delta_mol_L = delta_g_L / g_per_mol # units in mols
    fluxL = delta_kg_L / length_interval / (area*0.0001) # flux estimate in kg/m2/sec
    fluxL_umol = (delta_mol_L*1000000) / length_interval / (area * 0.0001) # flux estimate in umol/m2/sec

    # Quadratic regression
    quadratic_model <- lm(kg_gas ~ 
                            as.numeric(difftime(interval_timestamp, min(interval_timestamp), 
                                                units = "secs")) +
                            I(as.numeric(difftime(interval_timestamp, 
                                                  min(interval_timestamp), 
                                                  units = "secs"))^2))
    quadratic_results <- summary(quadratic_model)
    quadratic_beta <- coef(quadratic_results)[2]  # Extract the beta coefficient; kg/s
    delta_kg_Q  = quadratic_beta*(length_interval) # units in kg
    delta_g_Q = delta_kg_Q*1000 # units in g
    delta_mol_Q = delta_g_Q / g_per_mol # units in mols
    fluxQ = delta_kg_Q / length_interval / (area*0.0001) # flux estimate in kg/m2/sec
    fluxQ_umol = (delta_mol_Q*1000000) / length_interval / (area * 0.0001) # flux estimate in umol/m2/sec

    # Append the results to the dataframe
    results_df <- rbind(results_df, data.frame(
      start_timestamp = start_point,
      end_timestamp = end_point,
      hour_of_obs = round(end_point, "hour"),
      method = method,
      linear_beta = linear_beta, # kg/s
      quadratic_beta = quadratic_beta, # kg/s
      length_interval = length_interval, # total interval length in seconds
      delta_kg_L = linear_beta/length_interval, # units in kg
      delta_kg_Q  = quadratic_beta/length_interval, # units in kg
      fluxL_kgm2sec = fluxL, # flux estimate in kg/m2/sec
      fluxQ_kgm2sec = fluxQ, # flux estimate in kg/m2/sec
      fluxL_umolm2sec = fluxL_umol, # flux estimate in umol/m2/sec
      fluxQ_umolm2sec = fluxQ_umol  # flux estimate in umol/m2/sec
    ))
  }

  # Return the results
  return(results_df)
}

```

Try using custom function with conversion to mass, above:

```{r}
# fluxbot estimates:
results_mass_bot <- calculate_regressions_custom_intervals(bot_intervals, startend_bot_closest, "fluxbot")

# lgr estimates: ~5s to complete
results_mass_lgr <- calculate_regressions_custom_intervals(lgr_intervals, startend, "LGR")
```

# merge resulting datasets by nearest timestamp:

need to update with new datasets, produced in previous chunk, that are actual estimates

```{r}

results_mass_lgr_bot <- results_mass_lgr %>% 
  rbind(results_mass_bot) %>% 
  select(fluxL_umolm2sec, fluxQ_umolm2sec, hour_of_obs, method)

# visualize:
results_mass_lgr_bot %>% 
  ggplot(aes(hour_of_obs, fluxL_umolm2sec, color = method))+
  geom_point(alpha = 0.5)+
  theme_classic()+
  labs(x = "", y = "linear flux estimate umol/m2/sec")

# widen data to get R2:
results_mass_lgr_bot_wide <- results_mass_lgr_bot %>% 
  pivot_wider(names_from = method, values_from = c(fluxL_umolm2sec, fluxQ_umolm2sec))

results_mass_lgr_bot_wide %>% 
  na.omit() %>% # remove those LGR observations that do not have a paired fluxbot observation
  ggplot(aes(x=fluxL_umolm2sec_LGR, y=fluxL_umolm2sec_fluxbot))+
  geom_point(alpha = 0.5, color = "hotpink", size = 2)+  
  theme_classic()+
  labs(x="LGR linear estimate flux, umol/m2/sec", 
       y = "fluxbot linear estimate flux, umol/m2/sec")+
  stat_smooth(method = "lm", color = "darkred")+
  stat_regline_equation(label.x = 0.25, label.y = 6)+
  stat_cor(aes(label=..rr.label..), label.x = 0.25, label.y = 5.5)
```

Write datasets for use in other scripts:

```{r}
write.csv(results_mass_lgr_bot, "lgr_bot20_fluxestimates.csv")
write.csv(results_mass_lgr_bot_wide, "lgr_bot20_fluxestimates_wide.csv")
write.csv(lgr_bot20, "rawdata_lgr_bot20.csv")
```

```{r}
# quadratic vs. linear:

```

EXTRACTING INTERVALS: CUSTOM FUNCTION

```{r}
# library(dplyr)
# library(lubridate)

extract_intervals_iterative <- function(time_series) {
  result <- data.frame()  # Initialize an empty data frame to store the results
  
  time_series %>%
    filter(minute(as.POSIXct(timestamp)) >= 56) -> result

  
  return(result)

}
```

SUCCESS in EXTRACTING INTERVALS

```{r}

# Use the function to iteratively extract intervals
lgr_obsintervals <- extract_intervals_iterative(lgr_intervals)
bot_obsintervals <- extract_intervals_iterative(bot_intervals)

test <- extract_intervals_and_regression(lgr_intervals)
print(test)

# write:
write.csv(lgr_obsintervals, "lgr_observationintervalsonly.csv")
write.csv(bot_obsintervals, "bot_observationintervalsonly.csv")
```
